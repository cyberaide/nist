\section{Big Data Use Cases}\label{S:usecases}

The NIST Big Data Working group has identified 51 benchmarking examples for Big Data\cite{??} spanning application areas such as:

\begin{description}

\item[Government Operation:] National Archives and Records
  Administration, Census Bureau

\item[Commercial:] Finance in Cloud, Cloud Backup, Mendeley
  (Citations), Netflix, Web Search, Digital Materials, Cargo shipping
  (as in UPS)

\item[Defense:] Sensors, Image surveillance, Situation Assessment
  Healthcare

\item[Life Sciences:] Medical records, Graph and Probabilistic
  analysis, Pathology, Bioimaging, Genomics, Epidemiology, People
  Activity models, BiodiversityDeep Learning and Social Media: Driving
  Car, Geolocate images/cameras, Twitter, Crowd Sourcing, Network
  Science, NIST benchmark datasets

\item[The Ecosystem for Research:] Metadata, Collaboration, Language
  Translation, Light source experiments

\item[Astronomy and Physics:] Sky Surveys compared to simulation,
  Large Hadron Collider at CERN, Belle Accelerator II in JapanEarth,

\item[Environmental and Polar Science:] Radar Scattering in
  Atmosphere, Earthquake, Ocean, Earth Observation, Ice sheet Radar
  scattering, Earth radar mapping, Climate simulation datasets,
  Atmospheric turbulence identification, Subsurface Biogeochemistry
  (microbes to watersheds), AmeriFlux and FLUXNET gas sensors

\item[Energy:] Smart grid

\end{description}

In addition we have 81 student projects from classes taught at Indiana University on the topic of Big Data. 
From these examples we have examined on six of the NIST uses case projects to identify the technologies used as shown in Table~\ref{T:usecases}.

\begin{table}[htb]
  \bigskip
  \setlength\tabcolsep{2pt}
  \caption{
    Technology used in a subset of usecases. A \OK~indicates that the technology is used in the given project. See Table~\ref{T:usecase2} for details on a specific project. The final row aggregates \OK~across projects.}
  \label{T:usecases}
  \bigskip
  \begin{small}
    \begin{center}
      \resizebox{\columnwidth}{!}{
        \begin{tabular}{|c|*{25}c|}
          \hline

          ID & \rot{Hadoop} & \rot{Mesos} & \rot{Spark} & \rot{Storm} & \rot{Pig} & \rot{Hive} & \rot{Drill} & \rot{HDFS} & \rot{HBase} 
             & \rot{Mysql} & \rot{MongoDB} & \rot{RethinkDB} & \rot{Mahout} & \rot{D3 and Tableau} & \rot{nltk} & \rot{MLlib} 
             & \rot{Lucene/Solr} & \rot{OpenCV} & \rot{Python} & \rot{Java} & \rot{Ganglia} & \rot{Nagios} & \rot{zookeeper} & \rot{AlchemyAPI}
             & \rot{R} \\ \hline

          % ID  & Hadoop & Mesos & Spark & Storm & Pig & Hive & Drill & hdfs & HBase & msql & mongo & rDB & mht & D3/T & nltt & mllib & lu/slr & OCV & Py  & Java & Gngla & Ngs & ZK  & AlchemyAPI & R   
          N$_1$ & \OK    &       & \OK   &       &     & \OK  & \OK   &      & \OK   & \OK  &       &     &     &      &      &       &        &     &     & \OK  & \OK   & \OK & \OK &            &     \\ \hline
          N$_2$ &        & \OK   & \OK   &       &     &      &       &      &       &      &       &     &     &      &      &       &        & \OK & \OK &      &       &     & \OK &            &     \\ \hline
          N$_3$ &        &       &       & \OK   &     &      &       &      & \OK   &      & \OK   &     &     & \OK  & \OK  &       &        &     & \OK & \OK  &       &     & \OK & \OK        & \OK \\ \hline
          N$_4$ & \OK    &       & \OK   &       &     &      &       &      & \OK   &      &       &     & \OK & \OK  &      & \OK   & \OK    &     &     & \OK  &       &     & \OK &            &     \\ \hline
          N$_5$ & \OK    &       & \OK   &       &     &      &       &      &       &      &       &     & \OK & \OK  &      & \OK   &        &     &     & \OK  &       &     &     &            &     \\ \hline
          N$_6$ & \OK    &       & \OK   &       & \OK & \OK  &       &      & \OK   &      & \OK   &     & \OK & \OK  &      & \OK   & \OK    &     &     & \OK  &       &     & \OK &            &     \\ \hline
          count & 4      & 1     & 5     & 1     & 1   & 2    & 1     & 0    & 4     & 1    & 2     & 0   & 3   & 4    & 1    & 3     & 2      & 1   & 2   & 5    & 1     & 1   & 5   & 1          & 1   \\ \hline

        \end{tabular}
      }
    \end{center}
  \end{small}
\end{table}


\begin{table*}[htb]
  \caption{Dataset used in the various use cases.}
  \bigskip
  \label{T:usecase2}
  \begin{center}
    \begin{tabular}{|c|p{0.8\columnwidth}|p{0.8\columnwidth}|c|}

      \hline

      ID & Use Case & Dataset & \shortstack{Size\\(GB)} \tabularnewline \hline
      N$_1$ & Fingerprint Matching & Special Database 14 - NIST Mated Fingerprint Card Pairs 2 & 2.1 \tabularnewline \hline
      N$_2$ & NIST Human and Face Detection & INRIA Person Dataset & 0.96 \tabularnewline \hline
      N$_3$ & NIST Twitter Analysis & Twitter & - \tabularnewline \hline
      N$_4$ & NIST Analytics for Healtcare Data / Health Informatics & Medicare Part-B in 2014 from Center for Medicare and Medicaid Services (CMS) & 0.1 \tabularnewline \hline
      N$_5$ & NIST Spatial Big Data/Spatial Statistics/Geographic Information Systems & Uber & 0.2 \tabularnewline \hline
      N$_6$ & NIST Data Warehousing and Data Mining & TBD & - \tabularnewline \hline

    \end{tabular}
  \end{center}
\end{table*}




For them we have identified 19 roles to be defined as part of the
Fingerprint example, and 5 roles for the Face detection. Furthermore,
we have identified additional 27 roles for the other four NIST use
cases that are included in Twitter Analysis, Analytics for Healthcare
Data/Health Informatics, Spatial Big Data/Spatial
Statistics/Geographic Information Systems, and Data Warehousing and
Data Mining

In addition we analyzed 36 projects from a big data class taught at
Indiana University in Spring '16. Here we identified 41 roles in 45
projects showcasing a wide divergent use case scenario.  A similar
class thought in Fall '15 resulted in 62 roles while using 39
datasets.


\footnote{6 projects from the 51 NIST use cases
81 other projects surveyed as part of classes taught at Indiana University
Findings
We found 62 roles from which many are repeatedly used in various projects. 
}




\subsection{Fingerprint Matching (N$_1$)}

Fingerprint recognition refers to the automated method for verifying a
match between two fingerprints and that is used to identify
individuals and verify their identity. Fingerprints are the most
widely used form of biometric used to identify individuals. The
automated fingerprint matching generally required the detection of
different fingerprint features (aggregate characteristics of ridges,
and minutia points) and then the use of fingerprint matching
algorithm, which can do both one-to- one and one-to- many matching
operations. Based on the number of matches a proximity score (distance
or similarity) can be calculated. Furthermore, NIST is providing via
the the NIST Fingerprint dataset a special database. The goal for this
usecase is the following: given a set of {\it probe} and {\it gallery}
images, compare the probe images to the gallery images, and report the
matching scores.  The dataset comprises 54,000 images and their
metadata.  It uses MINDTCT \cite{mindtct} preprocesses the images
to identify minutae of the prints automatically locating and recording
ridge ending and bifurcations in a fingerprint image; and BOZORTH3
\cite{garris2001user} to identify matches. Both are are part of the NIST
Biometric Image Software (NBIS) \cite{watson2007user}.

To execute this usecase we need to deploy an application to analyze
the dataset. It internally uses cloudmesh to deploy an the software
stack The implemented \cite{nist-fingerprint}
solution uses a software stack comprising of HDFS, YARN, Apache Spark,
Apache HBase, and Apache drill, Scaka, and the NBIS software. A Hadoop
cluster is deployed and YARN used to schedule Spark jobs that load the
images into HBase, process the images, and compute the matches. Apache
Drill, with the HBase plugin, can then be used to generate reports
with the NBIS tools\cite{watson2007user}. The results are stored in HBase and
Apache Drill \cite{??} is used to query the results.  The code
leverages tools and services is based on \cite{nist-bd-pwg} \TODO{Hyungro: put
  this in the references: the “NIST Big Data Public Working Group
  draft Possible Big Data Use Cases Implementation using NBDRA
  authored by Afzal Godil and Wo Chang”.}  , while significantly
enhancing it with cloudmesh deployment strategies and services.

\cite{flanagan2010nist}
\TODO{Hyungro: create citation for this. http://www.nist.gov/itl/iad/ig/nbis.cfm}

\subsection{Face Detection (N$_2$)}

\TODO{Hyungro: define the references}

Human detection and face detection have been studied during the last
several years and models for them have improved along with Histograms
of Oriented Gradients (HOG) \cite{dalal2005histograms} for Human Detection. 



We use OpenCV \cite{bradski2000opencv}, a Computer Vision library including the
Support Vector Machine (SVM) classifier, and the Histogram of Oriented
Gradient (HOG) \cite{dalal2005histograms}object detector for pedestrian detection and
INRIA Person Dataset is one of popular samples for both training and
testing purposes. HOG with SVM model is used used as object detectors
and classifiers while the python libraries from OpenCV provide these
models for human detection.  The OpenCV Python code runs with Spark
Map function to perform distributed job processing on the Mesos
scheduler.

To enable this analysis we use cloudmesh to deployed Apache Spark on a
Mesos clusters and install the OpenCV software and its Python API. We
also update the python software stack. Then we to train and apply
detection models from OpenCV using Python API. We use the INRIA Person
Dataset \cite{dalal2005inria}. This dataset contains positive and negative images for
training and test purposes with annotation files for upright persons in each
image. 288 positive test images, 453 negative test images, 614 positive
training images and 1218 negative training images are included along with
normalized 64x128 pixel formats. The size of the dataset is 970MB.

Cloudmesh deploys and builds the clusters for batch-processing large
datasets, Internally cloudmesh uses for this ansible scripts to
support installation and configuration while leveraging available
cloud compute resources. We have for this example developed or are
reusing five ansible roles that we developed for other usecases::
Apache Spark \cite{ansible-role-spark}
Apache Mesos \cite{hindman2011mesos}, Apache Zookeeper
\cite{hunt2010zookeeper}, \TODO{Hyungro: make reference out of: Ansible Role
  for Mesos (with Zookeeper):
  https://github.com/VirtualClusters/ansible-role-mesos-by-mesosphere}
  OpenCV (with Python) \cite{ansible-role-opencv}



\cite{nist-facedetection}
\TODO{Hyungro: make reference out of: Ansible Main Play (Include Statement): https://github.com/futuresystems/pedestrian-and-face-detection}

\TODO{Hyungro: add also the original pointed and not just the ansible citation,
e.g. each should have to citations}


