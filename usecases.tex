\section{Big Data Use Cases}


\subsection{Overlap of Technologies}

6 projects from the 51 NIST use cases
81 other projects surveyed as part of classes taught at Indiana University
Findings
We found 62 roles from which many are repeatedly used in various projects. 

\subsection{Ansible for NIST deployments}

6 NIST examples
19 roles for Fingerprint 
5 roles for Face detection 
27 roles for the other four NIST use cases
Twitter Analysis
Analytics for Healthcare Data/Health Informatics
Spatial Big Data/Spatial Statistics/Geographic Information Systems
Data Warehousing and Data Mining
36 Projects from class, Spring '16 
41 roles
45 Projects from class, Fall '15
62 roles, 39 datasets



\subsection{51 NIST Benchmarking Examples for Big Data}

\begin{description}

\item[Government Operation:] National Archives and Records
  Administration, Census Bureau

\item[Commercial:] Finance in Cloud, Cloud Backup, Mendeley
  (Citations), Netflix, Web Search, Digital Materials, Cargo shipping
  (as in UPS)

\item[Defense:] Sensors, Image surveillance, Situation Assessment
  Healthcare

\item[Life Sciences:] Medical records, Graph and Probabilistic
  analysis, Pathology, Bioimaging, Genomics, Epidemiology, People
  Activity models, BiodiversityDeep Learning and Social Media: Driving
  Car, Geolocate images/cameras, Twitter, Crowd Sourcing, Network
  Science, NIST benchmark datasets

\item[The Ecosystem for Research:] Metadata, Collaboration, Language
  Translation, Light source experiments

\item[Astronomy and Physics:] Sky Surveys compared to simulation,
  Large Hadron Collider at CERN, Belle Accelerator II in JapanEarth,

\item[Environmental and Polar Science:] Radar Scattering in
  Atmosphere, Earthquake, Ocean, Earth Observation, Ice sheet Radar
  scattering, Earth radar mapping, Climate simulation datasets,
  Atmospheric turbulence identification, Subsurface Biogeochemistry
  (microbes to watersheds), AmeriFlux and FLUXNET gas sensors

\item[Energy:] Smart grid

\end{description}

\input{table_nist_projects_with_roles}


\TODO{write about how many examples we have and things like
  that. Take from presentation}

\TODO{define the index numbers U = usecase N = Nist}

\subsection{Fingerprint Matching (N$_1$)}

Fingerprint recognition refers to the automated method for verifying a
match between two fingerprints and that is used to identify
individuals and verify their identity. Fingerprints are the most
widely used form of biometric used to identify individuals. The
automated fingerprint matching generally required the detection of
different fingerprint features (aggregate characteristics of ridges,
and minutia points) and then the use of fingerprint matching
algorithm, which can do both one-to- one and one-to- many matching
operations. Based on the number of matches a proximity score (distance
or similarity) can be calculated. Furthermore, NIST is providing via
the the NIST Fingerprint dataset a special database. The goal for this
usecase is the following: given a set of {\it probe} and {\it gallery}
images, compare the probe images to the gallery images, and report the
matching scores.  The dataset comprises 54,000 images and their
metadata.  It uses MINDTCT \cite{mindtct} preprocesses the images
to identify minutae of the prints automatically locating and recording
ridge ending and bifurcations in a fingerprint image; and BOZORTH3
\cite{garris2001user} to identify matches. Both are are part of the NIST
Biometric Image Software (NBIS) \cite{watson2007user}.

To execute this usecase we need to deploy an application to analyze
the dataset. It internally uses cloudmesh to deploy an the software
stack The implemented \cite{nist-fingerprint}
solution uses a software stack comprising of HDFS, YARN, Apache Spark,
Apache HBase, and Apache drill, Scaka, and the NBIS software. A Hadoop
cluster is deployed and YARN used to schedule Spark jobs that load the
images into HBase, process the images, and compute the matches. Apache
Drill, with the HBase plugin, can then be used to generate reports
with the NBIS tools\cite{watson2007user}. The results are stored in HBase and
Apache Drill \cite{??} is used to query the results.  The code
leverages tools and services is based on \cite{nist-bd-pwg} \TODO{Hyungro: put
  this in the references: the “NIST Big Data Public Working Group
  draft Possible Big Data Use Cases Implementation using NBDRA
  authored by Afzal Godil and Wo Chang”.}  , while significantly
enhancing it with cloudmesh deployment strategies and services.

\cite{flanagan2010nist}
\TODO{Hyungro: create citation for this. http://www.nist.gov/itl/iad/ig/nbis.cfm}

\subsection{Face Detection (N$_2$)}

\TODO{Hyungro: define the references}

Human detection and face detection have been studied during the last
several years and models for them have improved along with Histograms
of Oriented Gradients (HOG) \cite{dalal2005histograms} for Human Detection. 



We use OpenCV \cite{bradski2000opencv}, a Computer Vision library including the
Support Vector Machine (SVM) classifier, and the Histogram of Oriented
Gradient (HOG) \cite{dalal2005histograms}object detector for pedestrian detection and
INRIA Person Dataset is one of popular samples for both training and
testing purposes. HOG with SVM model is used used as object detectors
and classifiers while the python libraries from OpenCV provide these
models for human detection.  The OpenCV Python code runs with Spark
Map function to perform distributed job processing on the Mesos
scheduler.

To enable this analysis we use cloudmesh to deployed Apache Spark on a
Mesos clusters and install the OpenCV software and its Python API. We
also update the python software stack. Then we to train and apply
detection models from OpenCV using Python API. We use the INRIA Person
Dataset \cite{dalal2005inria}. This dataset contains positive and negative images for
training and test purposes with annotation files for upright persons in each
image. 288 positive test images, 453 negative test images, 614 positive
training images and 1218 negative training images are included along with
normalized 64x128 pixel formats. The size of the dataset is 970MB.

Cloudmesh deploys and builds the clusters for batch-processing large
datasets, Internally cloudmesh uses for this ansible scripts to
support installation and configuration while leveraging available
cloud compute resources. We have for this example developed or are
reusing five ansible roles that we developed for other usecases::
Apache Spark \cite{ansible-role-spark}
Apache Mesos \cite{hindman2011mesos}, Apache Zookeeper
\cite{hunt2010zookeeper}, \TODO{Hyungro: make reference out of: Ansible Role
  for Mesos (with Zookeeper):
  https://github.com/VirtualClusters/ansible-role-mesos-by-mesosphere}
  OpenCV (with Python) \cite{ansible-role-opencv}



\cite{nist-facedetection}
\TODO{Hyungro: make reference out of: Ansible Main Play (Include Statement): https://github.com/futuresystems/pedestrian-and-face-detection}

\TODO{Hyungro: add also the original pointed and not just the ansible citation,
e.g. each should have to citations}


