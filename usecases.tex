\section{Big Data Usecases}

\TODO{write about how many examples we have and things like
  that. Take from presentation}

define the index numbers

\subsection{Fingerprint Matching (U.N1)}

Fingerprint recognition refers to the automated method for verifying a
match between two fingerprints and that is used to identify
individuals and verify their identity. Fingerprints are the most
widely used form of biometric used to identify individuals. The
automated fingerprint matching generally required the detection of
different fingerprint features (aggregate characteristics of ridges,
and minutia points) and then the use of fingerprint matching
algorithm, which can do both one-to- one and one-to- many matching
operations. Based on the number of matches a proximity score (distance
or similarity) can be calculated. Furthermore, NIST is providing via
the the NIST Fingerprint dataset a special database. The goal for this
usecase is the following: given a set of {\it probe} and {\it gallery}
images, compare the probe images to the gallery images, and report the
matching scores.  The dataset comprises 54,000 images and their
metadata.  It uses MINDTCT \cite{Hyungro:??} preprocesses the images
to identify minutae of the prints automatically locating and recording
ridge ending and bifurcations in a fingerprint image; and BOZORTH3
\cite{Hyungro:??} to identify matches. Both are are part of the NIST
Biometric Image Software (NBIS) \cite{Hyungro:??}.

To execute this usecase we need to deploy an application to analyze
the dataset. It internally uses cloudmesh to deploy an the software
stack The implemented \cite{??} \TODO{Hyungro: make refernce out of
  this
  https://github.com/cloudmesh/example-project-nist-fingerprint-matching}
solution uses a software stack comprising of HDFS, YARN, Apache Spark,
Apache HBase, and Apache drill, Scaka, and the NBIS software. A Hadoop
cluster is deployed and YARN used to schedule Spark jobs that load the
images into HBase, process the images, and compute the matches. Apache
Drill, with the HBase plugin, can then be used to generate reports
with the NBIS tools\cite{??}. The results are stored in HBase and
Apache Drill \cite{??} is used to query the results.  The code
leverages tools and services is based on \cite{??} \TODO{Hyungro: put
  this in the references: the “NIST Big Data Public Working Group
  draft Possible Big Data Use Cases Implementation using NBDRA
  authored by Afzal Godil and Wo Chang”.}  , while significantly
enhancing it with cloudmesh deployment strategies and services.

\TODO{Hyungro: create citation for this. http://www.nist.gov/itl/iad/ig/nbis.cfm}


\subsection{Face Detection (U.N2)}

\TODO{Hyungro: define the references}

Human detection and face detection have been studied during the last
several years and models for them have improved along with Histograms
of Oriented Gradients (HOG) \cite{??} for Human Detection. 



We use OpenCV \cite{??}, a Computer Vision library including the
Support Vector Machine (SVM) classifier, and the Histogram of Oriented
Gradient (HOG) \cite{??}object detector for pedestrian detection and
INRIA Person Dataset is one of popular samples for both training and
testing purposes. HOG with SVM model is used used as object detectors
and classifiers while the python libraries from OpenCV provide these
models for human detection.  The OpenCV Python code runs with Spark
Map function to perform distributed job processing on the Mesos
scheduler.

To enable this analysis we use cloudmesh to deployed Apache Spark on a
Mesos clusters and install the OpenCV software and its Python API. We
also update the python software stack. Then we to train and apply
detection models from OpenCV using Python API. We use the INRIA Person
Dataset \cite{??}. This dataset contains positive and negative images for
training and test purposes with annotation files for upright persons in each
image. 288 positive test images, 453 negative test images, 614 positive
training images and 1218 negative training images are included along with
normalized 64x128 pixel formats. The size of the dataset is 970MB.

Cloudmesh deploys and builds the clusters for batch-processing large
datasets, Internally cloudmesh uses for this ansible scripts to
support installation and configuration while leveraging available
cloud compute resources. We have for this example developed or are
reusing five ansible roles that we developed for other usecases::
Apache Spark \cite{??}, \TODO{Hyungro: make reference out of: Ansible
  Role for Spark:
  https://github.com/VirtualClusters/ansible-role-spark-for-mesos}
Apache Mesos \cite{??}, Apache Zookeeper \cite{??}, \TODO{Hyungro:
  make reference out of: Ansible Role for Mesos (with Zookeeper):
  https://github.com/VirtualClusters/ansible-role-mesos-by-mesosphere}
OpenCV (with Python) \cite{??}  \TODO{Hyungro: make reference out of:
  Ansible Role for OpenCV:
  https://github.com/futuresystems/ansible-role-opencv}



\TODO{Hyungro: make reference out of: Ansible Main Play (Include Statement): https://github.com/futuresystems/pedestrian-and-face-detection}

\TODO{Hyungro: add also the original pointed and not just the ansible citation,
e.g. each should have to citations}


