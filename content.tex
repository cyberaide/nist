\makeatletter
\setcounter{tocdepth}{3}
\def\contentsname{Contents}
\def\tableofcontents{%
    \section*{\MakeUppercase{\contentsname}}%
    \@starttoc{toc}%
    }
\makeatother
\newcommand{\AUTHOR}{Gregor von Laszewski, Badi Abdul-Wahid, Fugang Wang, Hyungro Lee, Geoffrey C. Fox}
\newcommand{\TITLE}{Cloudmesh in support of the\\NIST Big Data Architecture Framework}


\begin{document}


\newcommand{\TODO}[1]{\todo[inline,color=red!20]{#1}}

\TOC

\title{\TITLE}


\numberofauthors{2} 
\author{
\alignauthor
\AUTHOR\\
       \affaddr{Indiana University}\\
       \affaddr{Bloomington, IN}\\
       \email{laszewski@gmail.com}
\alignauthor
Wo Chang\\
       \affaddr{NIST Big Data Public Working Group}\\
       \affaddr{National Institute of Standards and Technology}\\
       \email{wo.chang@nist.gov}
}
\date{20 April 2013}

\maketitle



\begin{abstract}

  The National Institute of Standards and Technology (NIST) has
  provided a big data reference architecture and is currently
  attempting to validate that architecture. As part of our current
  efforts we have developed cloudmesh client a tool that sets its goal
  towards easily managing clouds, container, batch queues and bare
  metal infrastructure. It also allows the integration of defined
  software stacks that can be used to deploy complex and
  state-of-the-art frameworks with devOps tools. Cloudmesh is on
  purpose designed to be vendor agnostic. We evaluate in this paper
  two aspects. First, based on our rich experience with clouds and
  other infrastructures, {\it can we verify the NIST reference
    architecture from our point of view?} Second, {\it which
    limitations may exist in cloudmesh client that need to be
    addressed to potentially improve integration with the NIST
    efforts?}  We will see in this paper that cloudmesh validates the
  NIST big data architecture which itself motivated further
  improvements to cloudmesh that we have implemented.
\end{abstract}


\section{Introduction}

Commercial, academic, and government leaders agree about the potential
of {\em Big Data} to spark innovation, fuel commerce, and drive
progress. Big Data is the common term used to describe the deluge of
data in today’s networked, digitized, sensor-laden, and information
driven world \cite{nist-bd}. Desirable is as a vendor neutral,
technology- and infrastructure-agnostic conceptual model used to
examine related issues.  The focus of this document is on the NIST Big
Data Reference Architecture and identify lessons learned from
cloudmesh that can augment and verify this architecture through
practical use cases. In the next sections we provide some background
information to motivate our work and to introduce the two frameworks
influencing this paper. This includes cloudmesh \cite{las12-cloud}
\cite{github-cloudmesh-client} and the NIST Big Data Reference
Architecture \cite{nist-bd}.  

Our paper is structured as follows. We start by giving brief
introductions to the National Institute of Standards (NIST) Big Data
Reference Architecture (Section~\ref{S:NBDarch}) followed by a brief
introduction to Cloudmesh (Section~\ref{S:cmclient}).  We then provide
a more in depth description of Cloudmesh while focussing on CLoudmesh
abstractions useful for big data analysis (Section
\ref{S:abstraction}). Next we investigate some usecases brought
forward by the NIST Big Data Working Group and see how cloudmesh can
help deploying and executing them (Section~\ref{S:usecases}) .

We conclude our paper with observations made while implementing these
usecases impacting the NIST Big Data Architecture and also cloudmesh
(Section~\ref{S:conclusion})


\subsection{NIST Big Data Reference Architecture}
\label{S:NBDarch}

The NIST big data working group is exploring pathways forward in this
direction which can be leveraged by the community. The current result
reference architecture is summarized in \cite{nist-bd}. From this
document we gather that the {\it ``conceptual model, referred to as
  the NIST Big Data Reference Architecture (NBDRA), was crafted by
  examining publicly available Big Data architectures representing
  various approaches and products. Inputs from the other NBD-PWG
  subgroups were also incorporated into the creation of the NBDRA. It
  is applicable to a variety of business environments, including
  tightly integrated enterprise systems, as well as loosely coupled
  vertical industries that rely on cooperation among independent
  stakeholders. The NBDRA captures the two known Big Data economic
  value chains: information, where value is created by data
  collection, integration, analysis, and applying the results to
  data-driven services, and the information technology (IT), where
  value is created by providing networking, infrastructure, platforms,
  and tools in support of vertical data-based applications.''}  It
will produce a number of documents related to definitions
\cite{nist-bd-v1}, taxonomies \cite{nist-bd-v2}, use cases and general
requirements \cite{nist-bd-v3}, security and privacy
\cite{nist-bd-v4}, architectures white paper survey \cite{nist-bd-v5},
reference architecture \cite{nist-bd}, standards roadmap
\cite{nist-bd-v7}. In addition we expect NIST to work on an interface
definition document that is partially influenced by the work presented
in this paper.

One of the desired tasks is to identify existing frameworks and to
analyze how they correlate to the current reference architecture
\cite{nist-bd}. This is conducted in order to validate and if needed
to improve the architecture.  The current NIST big data reference
architecture(NBDRA) is shown in Figure~\ref{F:NIST-arch}. However we have
augmented the architecture with areas where cloudmesh interfaces with
it. We will describe the offerings in relationship to this
architecture of cloudmesh in detail in Section~\ref{S:cm-nist}.  The
NBDRA contains according to \cite{nist-bd} the following
components and services:

\begin{figure}[hp]
  \centering
     \includegraphics[width=1.0\columnwidth]{images/nist-bda.pdf}
  \caption{NIST Big Data Reference Architecture (NBDRA) diagram} 
  \label{F:NIST-arch}
%\end{figure}

\bigskip
%\begin{figure}[htb]
  \centering
     \includegraphics[width=1.0\columnwidth]{images/cloudmesh-arch-1.pdf}
  \caption{Cloudmesh layered architecture.} 
  \label{F:NIST-arch}
%\end{figure}
\bigskip

%\begin{figure}[htb]
  \centering
      \includegraphics[width=1.0\columnwidth]{images/cloudmesh-arch-2.pdf}
  \caption{Cloudmesh components.} 
  \label{F:NIST-arch}
\end{figure}

\begin{figure}[htb]
  \centering
      \includegraphics[width=1.0\columnwidth]{images/nist-devops-1.pdf}
  \caption{Continuous improvement while using cloudmesh interactively.}
  \label{F:NIST-arch}
%\end{figure}

\bigskip

%\begin{figure}[htb]
  \centering
      \includegraphics[width=0.8\columnwidth]{images/nist-devops-2.pdf}
  \caption{Interaction of the continuous improvement steps with
    various databases while using ansible deployment scripts.}
  \label{F:NIST-arch}
\end{figure}




\begin{description}

\item {\bf System Orchestrator} - provides high-level design dataflow
  between analytics tools and given datasets, computing system
  requirements, monitoring system resource and performance. At times
  the system orchestration is performed by the data scientist in an
  interactive manner. The system orchestrator is a service or
  component that acts in behalf of the data scientist, or another data
  science service that would require a particular configuration.

\item {\bf Data Provider} - provides abstraction of various types of 
data sources (such as raw data or data previously transformed by another system) 
and makes them available through different functional interfaces. This includes 
transfer analytics codes to data sources for effective analytic processing.


\item {\bf Big Data Application Provider} - provides analytics processing 
throughout the data lifecycle - acquisition, curation, analysis, visualization, 
and access - to meet requirements established by the System Orchestrator.


\item {\bf Big Data Framework Provider} - provides one or more instances 
of computing environment to support general Big Data tools, distributed file systems, 
and computing infrastructure - to meet requirements established by the Big Data 
Application Provider.


\item  {\bf Data Consumer} - provides interface to receive the value output 
from this NBD-RA ecosystem.


\item  {\bf Security and Privacy Fabric} - provides System Orchestrator 
the security and privacy interaction to the rest of the NBD-RA components to ensure 
protection of data and their content.


\item {\bf Management Fabric} - provides System Orchestrator the management 
interaction to the rest of the NBD-RA components with versatile system and software 
provisioning, resource and performance monitoring, while maintaining a high level 
of data quality and secure accessibility.

\end{description}




\section{Cloudmesh Client}
\label{S:cmclient}

The cloudmesh client toolkit is a lightweight client interface of
accessing heterogeneous clouds, clusters, and workstations right from
the users computer. The user can manage her own set of resources she
would like to utilize. Thus the user has the freedom to customize
their cyber infrastructure they use. Cloudmesh client includes an API,
a command-line client, and a command-line shell. It strives to abstract
backends to databases that are used to manage the workflow utilizing
the different infrastructure and also the services. Switching for
example to stage virtual machines from OpenStack clouds to amazon is
as simple as specifying the name of the cloud. Moreover, cloudmesh
client can be installed on Linux, MacOSX, and in future
Windows. Currently cloudmesh supports backends to SLURM, SSH,
OpenStack, AWS, and Azure. In the past we supported AWS and Azure.
Cloudmesh client allows to easily manage virtual machines, containers,
HPC tasks, through a convenient client and API. Hence cloudmesh is not
only a multi-cloud, but a multi-hpc environment that allows also to
use container technologies (under development). Additionally, we have
an example code on how to create a Web based portal with the help of
cloudmesh client.

\subsection{General Requirements and Goals}

Cloudmesh has from its inception followed the following general design
goals and requirements.

\begin{description}
\item [Technology agnostic.] An important aspect of cloudmesh is to
  offer access to useful services APIs and interfaces in a technology
  agnostic fashion. Thus it should be possible for example to switch
  easily between different IaaS providers. CM has excellently protected
  us during the changes of the OpenStack interfaces and libraries from
  the very first version of OpenStack. It also has allowed us to
  switch easily to different IaaS providers when it became clear that
  Eucalyptus was replaced by many with OpenStack.

\item[Easy to use.] Cloudmesh is supposed to make access of the
  complex workflow to integrate with IaaS and deploy on them new
  platforms and software stacks. This advanced feature is not only to
  be performed by expert IT personal or programmers, but in fact by
  data scientists which we found in practice have less experience in
  such areas. As we deal often with many compute nodes it is often
  insufficient to just provide a graphical user interface or portal, but
  we need to provide APIs, REST interfaces and especially command-line
  and shell access in an easy comprehensible fashion. 

\item[Expandable.] While working over the last years in the area it is
  obvious that the technology is rapidly evolving and new features
  need to be integrated, Thus it is important that cloudmesh is easy
  to expand and new features can be added while leveraging a core set
  of functionality and services.

\item[Documented.] Furthermore it is important that we provide from
  the start documentation to existing and new features and make it
  easy for the developers to contribute documented add ons, but also
  allow users to have access to documentation including examples. This
  will include easy to follow documented installation and configuration steps 
to guarantee successful deployment and use

\item[Repeatable and automated deployment.] When we install software
  stacks an a variety of platforms it is expected that that can easily
  be replicated and repeated automatically.

\item[Portable.] It is important to provide services in cross
  platforms compatible fashion. It ensures working, executable
  software deployment on multiple platforms. This includes not only
  the installation of the software, but the integration of external
  services and tools such as DevvOps or workflow frameworks that could
  support the general mission of a data scientist.

\item[Abstractions.] To address some of the design issues our
  requirements implicitly asks for the existence of a number of
  abstractions and interfaces that can be used to enable portable and
  crossplatform services and tools.

\end{description}

\subsection{Architecture Requirements and Goals}

In addition to the general requirements we set some specific
architectural requirements and goals that have been growing from
previous versions of cloudmesh and our earlier work in this area.

\begin{description}

\item{\bf Client based.} Cloudmesh client as the name indicates is a
  client based toolkit that is installed and run on the users
  computers. An add on component to use the client within a portal is
  available. Thus we distinguish the client that contains most of the
  functionality and the portal that can access the functionality
  through a locally maintaine Web portal. Important to note is that
  the user manages its own credentials and thus security and
  credential management is done directly on the users machine instead
  through a hosted Web portal. This increases the security as access
  to any credential is managed by the user and is not part of a
  credential management system.

\item{\bf REST.} Although Cloudmesh provides a client interface, it
will provide a REST interface to many of its services in order to
support service based deployments. The basic APIs developed for the
client can easily be reused to implement such interfaces.\footnote{we
  have demonstrated that cloudmesh APIs can be used to implement REST
  interfaces in a variety of frameworkks such as Flask, Django, and Cherypy}

\item{\bf Layered Architecture.} Cloudmesh client has a layered
architecture that allows easy development of new features. This also
allows contribution by the community while developing integrated and
smaller sub components. Figure A depicts the various layers. A
resource abstraction layer allows the integration of a multitude of
resources spanning HPC, Containers, and Cloud resources. (At this time
we focus on OpenStack and Slurm resources. We are working on
reintegrating resources such as Azure, AWS, Maui, Moab, and others
which we previously supported, as well as new resources such as
docker).

\item{\bf Management Framework.} Cloudmesh client contains a
management framework, and its components are depicted in Figure
B. cloudmesh allows easy management of virtual machines, containers,
and the data associated with them. We are currently developing a
choreography framework that leverages Ansible, chef, and heat. All of
the functionality is easily usable through a comand-shell that also
can be used from the command-line, and a Python API. IN future we will
be providing a REST API.

\item{\bf Database Agnostic.} Cloudmesh contains some state about the
resource and environment that a user may want to use. The information
is managed in an database abstraction that would allow storing the
data in a variety of databases such as SQL and MongoDB. At this time
we have chosen SQLite to be the default database as it does not
require any additional setup and is universally available on all
operating systems without change.

\item{\bf comand-shell and line.} Cloudmesh contains a comand-shell
allowing scripts to be developed and run. However we designed the
comand-shell in such a way that each command can also be called from
the command-line. Through the cloudmesh state machine the state
between comand-shell, command-client, and the portal is shared.

\item{\bf Cloudmesh Client Portal.} Previously, we distributed
cloudmesh with client, server, and a portal components in one
package. This however turned out to be to complex to be installed for
some of our less technically skilled user community. Thus we split up
the install into two independent packages. The cloudmesh client and
the cloudmesh portal. The portal provides some elementary features to
manage virtual machines and HPC jobs. At this time the portal is
considered to be alpha technology. Just as the client the portal is to
be run on the local user machine in oredr to allow increased security
by managing the credentials locally rather than on a server.

\item{\bf Cloudmesh Two Factor Authentication.} We have an
exploratory project in place that looks at the use of Yubikeys for
cloudmesh, client and cloudmesh portal.

\item{\bf Cloudmesh Comet.} We are actively developing the client
interface for SDSC’s comet supercomputer allowing bare metal
provisioning. The interface reuses cloudmesh components and
technologies while interfacing with the comet cloud REST
interface. The goal here is to manage virtual clusters.

\end{description}




\section{Cloudmesh Abstractions}\label{S:abstraction}

In this paper we will focus our attention to three important
abstractions that cloudmesh introduces: Cloudmesh Compute Experiments
(Section~\ref{S:experiments}) , Cloudmesh Virtual Clusters (Section
\ref{S:vc}), Cloudmesh Stacks (Section~\ref{S:stacks}).

\subsection{Compute Experiments} \label{S:experiments}

For many decades traditional supercomputing has provided large scale
resources to the research community. This is done in a shared
operating mode and enables researchers to utilize resources that they
otherwise would not have access to. Shared resources include
memory over a number of processors, shared disks, powerful processors
in speed and core numbers, fast interconnect. This has been applied to
many modeling applications but can naturally also be applied to big
data. In order to integrate such capabilities in a service oriented
fashion the Grid community has delivered prior to to e popularization
of cloudcomputing useful interfaces, API's, and toolkits. However for
research work that we supported with cloudmesh such interfaces and
efforts were to complex to use and we designed a simple model for
researchers that we worked with. This interface includes the abstract
concept of a ``job'' that is submitted to a queuing system and uses a
particular data set. The experiments in our case are repeated multiple
times and each run creates its own output directory.

Hence we have created a simple submission interface that submits the
script to the cluster. The script will be copied prior to execution
into the home directory on the remote machine. If a directory is
specified it will be copied into that dir.  The name of the output
directory is either specified in the script itself, or if not the
default naming scheme of cloudmesh is used using and the output
directory name is appended with an increasing index. To run such a
script we can on our client simply say
 
\begin{Verbatim}
    $ cm hpc run SCRIPT
\end{Verbatim}

Some more details about out improved interfaces are given in
Figure~\ref{T:rest}.

\begin{table*}[htb]
\caption{Selected Service Description}\label{T:rest}
\bigskip
\begin{center}
\begin{small}
\begin{tabular}{|l|l|l|}
\hline
\blue \textbf{Resource} & \blue \textbf{REST Method} & \blue \textbf{Description}\tabularnewline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Virtual cluster
\hline \multicolumn{3}{|l|}{\grey\bf Virtual Cluster: /cluster} \tabularnewline \hline
/                       & GET      & List available clusters \tabularnewline \hline
/                       & POST     & Launch a cluster on the provider \tabularnewline \hline
/                       & DELETE   & Delete all available clusters \tabularnewline \hline
/:id                    & DELETE   & Delete and destroy a cluster \tabularnewline \hline
/:id                    & GET      & View the status of a cluster (nodes, node type, etc) \tabularnewline \hline
/:id/properties/:property & GET, PUT & Get/set a property (provider, name, description) of the cluster \tabularnewline \hline
/:id/inventory/:format  & GET      & Obtain an inventory of the cluster \tabularnewline \hline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Composition
\hline \multicolumn{3}{|l|}{\grey\bf Stack Composition: /stack} \tabularnewline \hline
/                               & GET      & List available compositions \tabularnewline \hline
/                               & POST     & Create a new composition \tabularnewline \hline
/:id                            & GET      & Show information about the composition \tabularnewline \hline
/:id                            & DELETE   & Delete a composition \tabularnewline \hline
/:id/name                       & GET, PUT & Get/set the name of the composition \tabularnewline \hline
/:id/add?deployer=:?\&source=:? & POST     & Add a layer to the composition \tabularnewline \hline
/:id/layers                      & GET      & List layers of the composition \tabularnewline \hline
/:id/layers/:id                  & DELETE   & Delete the layer of the composition \tabularnewline \hline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Stack Deployment
\hline \multicolumn{3}{|l|}{\grey\bf Stack Deployment: /stack} \tabularnewline \hline
/                         & GET  & List the available stacks with discription \tabularnewline \hline
/:id/deployments/:cluster & POST & Deploy a stack onto a cluster \tabularnewline \hline
/:id/status               & GET  & Current status \tabularnewline \hline
/:id/deployments/:cluster & GET  & Current status on given cluster \tabularnewline \hline

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Batch
\hline \multicolumn{3}{|l|}{\grey\bf Batch Experiments: /hpc} \tabularnewline \hline
/                          & GET    & List all jobs started with the run command \tabularnewline \hline
/:id                       & DELETE & Deletes the experiment with the given id \tabularnewline \hline
/run?script=:?\&cluster=:? & POST   & Submits an experiment to the named cluster \tabularnewline \hline
/:id/status                & GET    & Returns the status of the job started with the run command \tabularnewline \hline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Files
\hline \multicolumn{3}{|l|}{\grey\bf File Connections: /connections/file \textcolor{red}{TODO}} \tabularnewline \hline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Database
\hline \multicolumn{3}{|l|}{\grey\bf Database Connections: /connections/db \textcolor{red}{TODO}} \tabularnewline \hline

\end{tabular}
\end{small}
\end{center}
\end{table*}




\subsection{Cloudmesh Virtual Clusters} \label{S:vc}

Traditional HPC provided clusters to the community while  the clusters
are managed by professional staff and have mostly rigid software
stack targeting a broad community. Integration specialized software
stack and accessing the newest development based software is often
difficult or impossible to manage for a large number of
communities. Thus we see the following advantages and disadvantages:

\begin{description}

\item[Advantages:] provides well defined environment to the community,
  provides often optimized services for this particular system or the
  community it serves, allows sharing of resources through queuing
  system

\item[Disadvantages:] Different communities, groups or projects may
  have different requirements that are not met by the software stack,
  software stack often not state-of-the art, but tuned for
  reliability, experimentation with new software and methodologies in
  such an environment is difficult, queuing system may not provide
  enough interactivity

\end{description}

 Hence we are in the need of a mechanism to provide
clusters in a more manageable form. With the advent of virtualization
technologies, this can be achieved while users are provided with
virtual clusters hosted on cloud infrastructure using virtual machines,
or container virtualization software. This provides us with the
following advantages and disadvantages:

\begin{description}

\item[Advantages:] user stack provide a a more flexible and
  state-of-the-art environment; support different interaction modes
  with instantaneous access without wait time; support the use of
  heterogeneous platforms to allow increase in availability as well as
  features; and support the easy management of such an environment

\item[Disadvantages:] the user needs to manage the stacks themselves; a
  steep learning curve is needed to achieve this; the environment are
  feature rich and are difficult to manage

\end{description}

Naturally, cloudmesh is targeting to ease the disadvantages laid out.

\subsection{Cloudmesh Groups} \label{S:groups}

One of the essential abstractions in cloudmesh is the definition of
groups on which actions can be performed. This allows us to
agglomerate named objects into a group where the objects could have
even different types. An example would be the resources that are part
of a virtual cluster and could include virtual machines, object stores,
or even queuing system services. On such groups we can apply actions
that work in three different modes. Either individually, in groups or
subgroups, or in an orchestrated fashion through workflows.

This simple abstraction makes it possible to quickly assemble high
level representations of virtual clusters than can be deployed,
monitored and managed by appropriate software. 


\begin{table*}[htb]
\caption{Additional abstractions}\label{T:group}
\bigskip
\begin{center}
\begin{small}
\begin{tabular}{|l|l|l|}
\hline
\blue \textbf{Resource} & \blue \textbf{REST Method} & \blue \textbf{Description}\tabularnewline

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Batch
\hline \multicolumn{3}{|l|}{\grey\bf Groups: } \tabularnewline \hline
/                          & GET    & List all groups \tabularnewline \hline
/:id                       & DELETE & Deletes the group with the given id \tabularnewline \hline
/:id/add/...               & POST   & Add elements to the group \tabularnewline \hline
/:id/status                & GET    & Returns the status of elements
                                      in the group \tabularnewline \hline
/:id/delete                & GET    &deletes elements
                                      in the group \tabularnewline \hline
/:id/delete/:member        & GET    & delete a member from the group
                                      in the group \tabularnewline \hline
/:id/list                  & GET    & list the elements in the group
                                      in the group \tabularnewline \hline
/:id/list/:member          & GET    & list the element in the group
                                      in the group \tabularnewline
                                      \hline
/:id/run/:action           & GET    & apply an action on a group.  \tabularnewline
                                      \hline
/:id/attach/:order         & GET    & apply an action on a group.  \tabularnewline
                                      \hline
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Additional abstractions
\hline \multicolumn{3}{|l|}{\grey\bf Groups: /group} \tabularnewline \hline
/                   & GET    & List all groups \tabularnewline \hline
/:id                & GET    & Returns the status of elements in the group \tabularnewline \hline
/:id                & DELETE & Deletes the group with the given id \tabularnewline \hline
/:id/member         & POST   & Add a new member to the group \tabularnewline \hline
/:id/member         & GET    & List the members of a group  \tabularnewline \hline
/:id/member/:member & PATCH  & Update a member from the group in the group \tabularnewline \hline
/:id/member/:member & DELETE & Deletes elements in the group \tabularnewline \hline

\end{tabular}
\end{small}
\end{center}
\end{table*}

\subsection{Uniform Access Interfaces}

\begin{description}

\item[Versioning.] In case different versions are uses the version number is a prefix to
the general query

\item [Info.] At time we may not only return results offered by the service that we
contact, but receive information about the service itself, this si
achieved with the info url. This has been introduced for the first
time in \cite{las02infogram}

\item [Query.] a query returns a subset of items. What kind of queries
  may be allowed may be communicated through an info command.

\item [Pageing.] In some instances it is important to restrict the returned values
which can be achieved with paging

\end{description}

\begin{Verbatim}
?offset=15&limit=5>
\end{Verbatim}

\subsection{Virtual Clusters}

Definition: A set of compute, storage and network services that
interact with each other to serve an application or community for a
specific amount of time to support one or more experiments. A virtual
cluster is managed by the community or application user. They are
typically built on top of HPC, Grids, Clouds, and Containers. The
virtual cluster is managed by the user.

Contrasting Grids: targeted towards the support of a virtual
organization introducing high overheads on the deployment and
management of such infrastructure. Grids are a natural expansion of
traditional supercomputer centers that are managed by professional
staff. Contrasting Cloud IaaS: offer typically low level IaaS services
allowing users to combine them. They are a valuable building block for
virtual clusters. Clouds are managed by professional staff.

Contrasting Cloud PaaS: offer enhanced services to users targeting a
particular platform. They are offered by professional staff.

Contrasting Containers: offer an abstraction to share the existing hardware and OS while using containers making the it not necessary to us OS virtualization, thus saving space. Containers provide very useful enhancements for creating virtual clusters through add ons such as Kubernetes and Docker Swarm.



\subsection{Cloudmesh Access and Management of virtual clusters}
Cloudmesh is an API, command line tool, and command shell allowing the easy utilization of HPC, Clouds, Containers through machine abstractions
Switching between alternative services can be achieved by updating a
single variable.

Demonstrated usages of
Comet                                                                                           (HPC integrated Cloud)
FutureSystems, Chameleon, CloudLab, Bridges, Jetstream, …  (National Openstack Clouds)
Cybera (CA), Karslruhe Openstack Cloud (KIT, Germany), …     (International)
EC2, AWS, Azure                                                                          (Commercial Non Openstack)
Devstack, Trystack, Virtualbox                                                       (Desktop Clouds)
A user could use all of them

\subsection{Cloudmesh and Comet Cloud Virtual Cluster}

Comet is a NSF sponsored super computer offered by SDSC to the
community. It operates in two modes: HPC and virtual clusters Comet
virtual clusters are low level and offer the HPC administrator the
view of a cluster as if it were hardware. This is achieved via
virtualization and low level exposure of network services. The
administrator has full control over the cluster.  This is achieved by
the integration of virtualization and SRIOV within Rocks. Thus the
same cluster can not only be used in virtualized mode, but also in HPC
mode Comet users can therefore use HPC and virtualized clusters on the
same hardware. Virtual clusters are treated as a special kind of
compute job Based on our long experience in this field we have
delivered to SDSC an extension to cloudmesh that allows the creation
and management of virtual clusters within comet. A subset of commands
that showcase how easy the interaction with comet is shown in Table \ref{T:comet}

% We can today use more than Comet
% Accessing Virtual Clusters
% REST API
% Command line interface
% Command shell for scripting
% Console Access
% Web Page

\begin{table*}[htb]
\caption{Commands to interact with XSEDE comet virtual machine
  management}\label{T:comet}
\begin{center}
\begin{tabular}{|l|l|}
\hline
Command & Description\\
\hline
\hline
cm comet cluster  ID
  &  Show the cluster details \\
cm comet power on ID vm-ID -[0-3] --walltime=6h
  & Power 3 nodes on for 6 hours \\
cm comet image attach image.iso ID vm-ID-0
  &  Attach an image \\
cm comet boot ID vm-ID-0
  & Boot node 0 \\
cm comet console vc4
  & Start interactive console for vc4  \\
cm var cloud=jetstream / bridges / aws / comet
   & easy switching of infrastructure \\
cm boot
   & booting of a virtual machine with default parameters\\
\hline
\end{tabular}
\end{center}
\end{table*}




\subsection{Cloudmesh Stack} 
\label{S:stacks}

Important is that cloudmesh introduces an abstraction for creating and
managing software stacks. Such stacks are used to tackle the problem
of reproducibly deploying software stacks and packages on virtual
clusters. They are also important to define customization of the
software stack that typically can not be provided by traditional
supercomputing centers and have become an integral part of cloud
computing and development efforts of large scale open source software
projects.

A {\it Stack} is a way to modify a collection of accessible resources
in order to bring them to a desired state. It is desirable that his
execution unit can interface with various deployment tools and
approaches. This includes scripts, programs, ssh, Make, OpenStack Heat
\cite{??}, Ansible \cite{??}, Chef \cite{??}, Puppet \cite{??}, Salt
\cite{??}, Vagrant \cite{??}, Dockerfile \cite{??}, and NixOS/NixOps
\cite{??}. The reason for this is that users can leverage their own
preferred deployment tools, but also allows uses to leverage efforts
conducted by the open source community while integrating large and
complex software stacks.  Hence, if available a variety of different
launch platforms can be integrated into cloudmesh and work from
different community contributions can be supported.

These deployment units are then composed as part of a stack {\it
  Composition} into layers to form a new deployment unit that can be
used as a layer within a different composition (illustrated in 
Figure~\ref{F:stack-composition}).


\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{images/cloudmesh-stack-composition.pdf}
\caption{Multiple stacks may be composed. Here, the
  {\it Fingerprint} stack is a composition of the Apache
  {\it Drill}, {\it HBase}, and {\it Spark}
  stacks. The {\it Fingerprint} stack is then included as a layer withing
  another composition built upon {\it Hadoop} and extended with the
  {\it Web} layers. The {\it Web} layer itself may be composed
  of other layers.
  \label{F:stack-composition}}
\end{figure}

To support the goals of a sophisticated deployment framework, stack
need to support the following properties:

\begin{description}
\item [Idempotent.]  Deployment of a stack should only bring the
  system to the desired state: applying a stack multiple times should
  be indistinguishable from applying it a single time. This also
  provides a measure of fault tolerance to the deployment.
\item [Reproducible.] Deployments of a given stack onto multiple
  identical targets should yield equivalent systems.
\item [Self contained.] All components need to bring a system to the
  stack's desired state should be declared by the stack.
\item [Configurable.] When a stack a defined default values may be
  assumed to take effect on deployment. In order to allow a system to
  be tuned to the desired state, depending on the user's requirements,
  these values must be configurable.
\item [Composable.] The semantics of deploying two stacks should be clear.
\item [Linkable.] Stacks should be able to link to each other to
  define choreography roles and avoid duplication.
\item [Extensible.] Stacks need to be easily extensible in order to be
  adapted to specific deployment needs.
\end{description}

To support this effort we define a cloudmesh {\em stack} command {\it
  stack} command that deploys and configures a user-defined subset of
the available modules while leveraging existing deployment
technologies.

In addition, Cloudmesh provides a number of predefined stacks such as
Apache Spark and Apache Drill that can be reused and customize virtual
clusters while enhancing them with sophisticated software stacks. 

This results in a minimization of work by the user as
they can leverage a number of existing deployments that are
available through open source repositories. Hence cloudmesh integrates
valuable contributions form the community while not only pointing to
them, but also making selected stacks available while vetting and
improving them \cite{www-cm-stacks}.


The stack command was initially developed to aid students and
researchers in completing data analytics projects.  They were faced
with exploring various technologies and would get stuck during the
installation and configuration phase, unable to progress due to the
complexity involved in managing it. While making cloudmesh stack
available and targeting often used deployment stacks we wer able to
assist these groups significantly.  

To illustrate the simplicity of use of sophisticated stacks that would
be otherwise not achievable by many users we provide the following example.

In this example we will create and deploy the stack shown in
Figure~\ref{F:stack-graph} that deploys one of our test use cases
described in Section \ref{S:fingerprint}. To achieve this we use the
commands listed in Figure~\ref{F:stack-deploy-fingerprint} and create
the {\it fingerprint} stack from preexisting {\it spark}, {\it hbase},
and {\it drill} stacks and compose it with the predefined {\it hadoop}
and {\it web} stacks. Then, a cluster of four nodes is launched onto
which the newly created stack is deployed. We use indentations to
illustrate better the association of a stack containing a composition
of layers.\footnote{the term layer may need to be replaced with the
  term add. We also need to document that the last stack is used by default.}

\begin{figure}[htb]
\begin{Verbatim}
    cm stack create my-analytics-stack
       cm stack layer hadoop
    cm stack create fingerprint
       cm stack layer spark hbase drill
       cm stack layer web
       cm cluster create --count 4
    cm stack deploy
\end{Verbatim}
\vspace{-12pt}
\caption{Commands to deploy the fingerprint $N_1$ stack}
\end{figure}

\subsubsection{Stack Composition} \label{S:composition}
\TODO{Badi}

A {\it Composition} creates a new stack by combining multiple other
stacks.  The unit of deployment is a {\it Stack}, but each stack may
be a composed of one or more layers of other stacks, forming a graph
structure whose evaluation results in deployment onto the target
cluster.

\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{images/cloudmesh-stack-graph.pdf}
\caption{Evaluation of a stack.
  The {\it My Analytics Stack} is composed of the {\it Fingerprint} and {\it Web} layers.
  These in turn are themselves compositions.
  Numbers indicate order of evaluation.
  While {\it spark}, {\it hbase}, and {\it drill} depend on {\it java}, 
  re-evaluation (\textit{4} - \textit{6}) is not problematic due to the idempotency property.
  \label{F:stack-graph}}
\end{figure}

Figure~\ref{F:stack-graph} illustrates how Figure~\ref{F:stack-composition} (while expanding on it) may be evaluated.
Evaluation of {\it My Analytics Stack} requires the evaluation of the
{\it Fingerprint} and {\it Web} stacks, each of which are compositions of other stacks.


\subsubsection{Stack Linking} \label{S:stack-linking}
\TODO{Badi}

\subsubsection{Language for Interpreted Stack Programming} \label{S:stack-lisp}
\TODO{Badi, title}


Building virtual clusters is easy

\begin{Verbatim}
cm launcher cluster -n 10
\end{Verbatim}

Gives a virtual cluster with 10 nodes and allows the user to login between each node

\begin{Verbatim}
cm launcher hadoop -n 10 --cloud=rackspace
\end{Verbatim}

Finds the playbook for creating a hadoop cluster and launches it on a cloud called rackspace

Building enhance virtual clusters is easy
Virtual cluster for face detection

\begin{Verbatim}
cm launcher facedetection -n 10 --cloud=rackspace
\end{Verbatim}

Finds the playbook for creating a virtual cluster and launches the
deployment for the face detection project.  Cloudmesh will look in the
git repository to locate the ansible scripts and execute them



----



Figure~\ref{F:cm-stack-command} ...

\begin{figure}[htb]
%\begin{lstlisting}
%[fontfamily=helvetica]
\begin{Verbatim}
  stack check [--stack=bds]
  stack init [--no-activate] 
             [--branch=master] 
             [--user=$\$$USER] 
             [--name=<project>] <ip>...
  stack list [--sort=<field=date>] 
             [--list=<field,...=all>]
  stack project [<name>]
  stack deploy [<play>...] 
               [--define=<define>...]
\end{Verbatim}
%\end{lstlisting}
\caption{Excerpt of the Cloudmesh stack manual page}
\label{F:cm-stack-command}
\end{figure}


\subsection{Create a hadoop cluster}

\begin{Verbatim}[fontfamily=helvetica]
$ cm hadoop start --user cc --name=cluster_1
$ cm delete --name=cluster_1 --all  
\end{Verbatim}


\section{Cloudmesh Big Data Stack}\label{S:cm-bds}

The Cloudmesh big data stack commands are leveraging abstractions and
can therefore implemented while utilizing and interacting with various
software packages from different layers. This enables us to accomplish
diverse tasks that have to be achieved in order to assist the
scientists trying to use big data as part of their application
missions. While providing them easily to the scientists complex big
data tasks can be achieved more easily.

\subsection{Devops}


\subsection{Playbooks and Roles}

BDS is a collection of playbooks for deploying the tools of big data
analytics, given a set of accessible IP addresses. It allows the user
to select a subset of the available tools to deploy in order to
customize their virtual cluster.

Example: to setup a stack with Hadoop, Spark, and Hive:
\begin{Verbatim}
$ ansible-playbook play-hadoop.yml addons/spark.yml addons/hive.yml
\end{Verbatim}

Example: to setup a stack with Hadoop, HBase, and Drill:

\begin{Verbatim}
$ ansible-playbook play-hadoop.yml addons/hbase.yml addons/drill.yml
\end{Verbatim}

It is provided online at https://github.com/futuresystems/big-data-stack

\begin{description}

\item[Roles:]
Drill,
Ganglia,
Hadoop,
Hbase,
Hive,
Java,
Limits,
Maven,
Mysql,
Nagios,
Pig,
Spark,
Supervisord,
Zookeeper

\item[Playbooks:]
Hadoop
Hbase
Pig
Spark
Drill
Hive
\end{description}

\subsection{Towards Cloudmesh}

BDS is ideally suited to be included in cloudmesh as one of its
deployment management tools. Cloudmesh adds features to easily access
infrastructure. ... more in the next another section we present later

Face Detection Screenshot Running EC2 provisioning playbook
on command line:

\begin{Verbatim}
$ ansible-playbook boot-ec2-with-vpc.yml -i ec2.py
\end{Verbatim} 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{usecases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Status}


We can replicate on different infrastructures including AWS, Azure,
Openstack, and XSEDE/SDSC Comet.

The {\it stack} command has been implemented, while we used to focus initially
on heat we found thia was a too limiting approach. Since than we have
expanded our activities on using for many of the stacks ansible and
provide a significant set as part of our open source solutions. This
includes:

\TODO{add some of teh stacks here even if replicated from above}

Cloudmesh is also used in the production system of XSEDE comet and is
used to interact with virtual clusters on that system showcasing the
general approach that we take in cloudmesh to integrate with vastly
different clouds and IaaS frameworks.

\section{Conclusion}\label{S:conclusion}

\TODO{add conclusion}

\subsection{Conclusion}
Ansible + Cloudmesh
Will enable re-usable specification of Big Data Stack applications in 87 use cases and 62 unique roles from which NIST has 27
BDS status
From this a total part of BDS are vetted (the most fundamental ones): 
14 Roles  (about 50\% done of NIST roles)
6   Playbooks



\subsection{Future Work}

\TODO{discuss additional work}

\TODO{server less computing \cite{hil16}} 

\TODO{server less computing \cite{cloud3}} 
